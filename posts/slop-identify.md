In 2023, Dr. Jared Mumm, a professor at Texas A&M University [denied several seniors their diplomas](https://www.rollingstone.com/culture/culture-features/texas-am-chatgpt-ai-professor-flunks-students-false-claims-1234736601/) after they had already walked.

These students would be receiving an "X" in the course, as the professor asserted they had written papers with "Chat GTP" (the correct name is "ChatGPT"). Dr. Mumm backed up his claim by showing he had asked "Chat GTP" if these students' work was written by it. Gleefully it claimed the credit (can't say it's not a go-getter). 

Thankfully he had "tested" each paper in this manner twice, so he felt confident in putting these students' futures on the line. Just a small snag, however... as his dissertation on pig farming written several years before the introduction of "Chat GTP" also appears to be written by it, at least according to it. It also took credit for wholly drafting the same email that Dr. Mumm had sent to the students letting them know they had been "caught".

While the ridiculousness of his claims were easy to test and the students were able to prove their work (they got their diplomas) using Google Doc timestamps logged as they wrote their papers, this terrifying anecdote raises important questions about the problems of authenticity in work today and if we have the capability to discern if something was written with AI-assistance or not.

For the digital world this is a quandary. AI content, colloquially referred to as "slop", can be pushed out in epic volumes much like the billions of spam emails sent every day. It's not currently possible for organizations to detect such content reliably, and they will not be able to avoid dealing with it.

They must adapt.

### Meta

Meta, the owner of Instagram and Facebook, [attempted to label AI generated images across their platforms](https://about.fb.com/news/2024/02/labeling-ai-generated-images-on-facebook-instagram-and-threads/). However, their system [incorrectly labels real photos and lets AI generated imagery through](https://www.techradar.com/cameras/photography/instagram-is-tagging-real-photos-as-made-with-ai-and-photographers-arent-happy). While there are heuristics that may hint at a photo's true origin (for example, by examining "metadata" within the file), a system utilizing these will likely not be infallible.

It's worth seriously considering what it means if Meta's products today cannot discern with certainty between generated content and human-authored content. Meta may have already considered the implications, and are discussing filling their [surfaces with generated content, even planning fake friends to converse with](https://www.wsj.com/tech/ai/mark-zuckerberg-ai-digital-future-0bb04de7?gaa_at=eafs&gaa_n=ASWzDAhDpTQCsAOr-WioewDjnwS3nXEg2UejWsUPxXr2Md9CW35SSLJBI3_zEAF82wQ%3D&gaa_ts=687ade2a&gaa_sig=nJF7u5zhILB7BybJa4LLLJ5fGyDzEcpG77VYPaqyzPpVePLMq67B-YnWJn4N0jXDDVPyW6GuxO0coslDuLw1FQ%3D%3D). It's possible that in the future many posts you see on some parts of Instagram/Facebook could be mass generated by AI.

This is not an unintelligent approach! If it's not possible for them to discern or filter this content, then the same likely goes for customers. Perhaps customers will find generated content engaging on its merits because by definition, it may be at human-quality. It turns out that there is significant overlap between some of the best quality output that AI can come up with and the worst quality output of humans. Perhaps slop gets a bad rap?

### School
While Meta might gleefully welcome a flood of new content to keep customers engaged, not everybody has the priviledge of leaning in. Schools are not as enthused, and face the following:

* AI threatens coursework
* Currently there is no reliable way to detect AI use in homework
* AI learning tools have a lot of potential

Also while many intellectuals posit about ["Zero Trust Homework"](https://stratechery.com/2022/ai-homework/#Zero_Trust_Homework) schemes to design new AI software systems with accountable generation histories and even purposeful errors, the unfortunate reality is that easy access to other models is here to stay.

To encourage students to write their essays with AI defeats the purpose of practicing their authorship and writing ability. It robs them of their own unique voice and robs them of their learning. There are already compelling studies showing that [using AI tools decreases activity throughout the brain](https://time.com/7295195/ai-chatgpt-google-learning-school/). Meanwhile tech companies such as Google are also eager to capture more traffic and happily advertise the benefits of using generative AI inside of search results that may be related to homework (parents should become aware of this).

While it's possible that generative models use [punctuation differently](https://www.washingtonpost.com/technology/2025/04/09/ai-em-dash-writing-punctuation-chatgpt/) than most people, there is no reliable way for an educator to discern if content was wholly generated with AI, let alone partially and edited. We cannot reliably filter between one student's toil and another student's AI-slop.

Where education gets interesting is the introduction of AI learning tools. When you're not using them to do your homework, AI tools by some measures are a transformative development in personalized tutoring. For example, [Duolingo recently introduced 'real life' conversations](https://blog.duolingo.com/video-call/) with an voice-enabled AI to practice your foreign language skills. I've personally used it and it is surreal. Like a real person it will converse with you, repeat words, and correct you. Practice makes perfect and with these tools, practice is more democratized.

This is not exclusive to language learning - while immensely popular youtube-tutor services like Khan Academy have been utilized by students for over a decade now, a generative AI chatbot can walk you through anything in great detail. For example starting a garden, testing soil ph, picking plants, understanding climate zones, and increasing your yields in a digestible, auditable way. I personally used it to level up my homebrew set up for creating beer: it explained the sparging process (don't worry about it) with ease and even let me know the minimum equipment necessary to purchase to accomplish it.

Parents should as always be active in their child's education. If we navigate this well, we just might see the smartest cohort of students to walk this Earth, who can benefit from an invention of personal digital tutors.

### Work
According to [this Gallup poll](https://www.gallup.com/workplace/651203/workplace-answering-big-questions.aspx), 33% of employees say their workplace has taken action to integrate AI, with 44% of white collar workers agreeing with the statement. People are using it, so let's discuss some of the high level effects and best practices. There are a few key takeaways I'd like to highlight:

* Below average employees benefit most from generative AI
* In places with clear bans, employers likely cannot stop employees from using them
* Businesses will overly trust AI

According to the poll, 45% of employees say that "Productivity and efficiency" has improved, but when combined with another interesting survey, you see an impressive pattern. [Below average performers are uplifted](https://www.ethicallyalignedai.com/post/below-average-workers-will-benefit-the-most-from-using-ai) by the availability of such tools, according to [study by the Harvard Business School](https://www.hbs.edu/faculty/Pages/item.aspx?num=64700). Below average workers saw a 43% improvement in their output vs a 17% gain for high performing counterparts. That is a huge win.

That said, while many business encourage these tools many ban AI over slop or data security concerns. Many [employees ignore these rules](https://www.welcometothejungle.com/en/articles/using-ai-secretly-at-work). A staggering 68% of respondents who use AI at work in a fishbowl survey say they don't disclose their specific usages of generative AI to their boss. 

Hallucinations (where bots make things up) are a danger inherent to the tools and will be a part of them for foreseeable future. Before you automate functions of your business, ensure you aren't connecting customers to a slop faucet as Air Canada did when they replaced customer chat with a bot making promises they couldn't keep - [and were held liable.](https://www.bbc.com/travel/article/20240222-air-canada-chatbot-misinformation-what-travellers-should-know) Remember, sometimes it is the [boss who is fired for secretly using AI, and naively producing slop](https://www.cnn.com/2023/12/11/media/sports-illustrated-ai-articles-ceo/index.html).

To prevent these fiascos from happening you need to make sure your bot can't unilaterally affect large parts of your business. It is possible to engineer guard rails. When using it in your communication or output, be extra wary to review its work as if it were written by an over-eager junior employee. There are many studies on the pacifying effects of AI systems generating over-reliance and [reducing critical thinking](https://slejournal.springeropen.com/articles/10.1186/s40561-024-00316-7) which can lead to more errors. It will require a new kind of awareness working with such a "partner" to steer their firehoses well in the new marketplace.

### In Sum
If we can't tell authenticity on social media, we can't do so in the classroom. If we can't stop students from using AI when they have access to the internet, we can't stop employees from doing so on the job. The right tools must be deployed officially in the workplace - lest workers find them on their own.

The question isn't "should we use AI?", and it isn't "how do we detect AI in our organization?". It's here to stay - the question is how to adapt.

A tool as powerful as AI can be leveraged to multiply output massively, and neither employers nor teachers will be able to tell if work was generated with assistance - will your organization's output be quality, or will it be slop?
